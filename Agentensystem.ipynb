{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvNyxOCwLL6UdExYbyWKKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Videothek/machine-learning/blob/main/Agentensystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF pytesseract Pillow langchain-community langchain_openai docarray faiss-cpu\n",
        "!apt-get update\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "id": "Bug4DF9pErI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definieren relevanter Variablen"
      ],
      "metadata": {
        "id": "hQDZmwnVFG1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name des Google-Colab-Secret für den OpenAI-Key\n",
        "colab_secret = \"oai_apikey\"\n",
        "\n",
        "# Name des Google-Colab-Secret für den OpenWeather-Key\n",
        "ow_colab_secret = \"ow_apikey\"\n",
        "\n",
        "# OpenWeather URL um die Wetterdaten abzurufen\n",
        "WEATHER_BASE_URL= \"https://api.openweathermap.org/data/2.5/weather?\"\n",
        "\n",
        "# Festlegen der URL von der das PDF-Dokument geladen werden kann.\n",
        "url = \"https://raw.githubusercontent.com/Videothek/machine-learning/main/MCP.pdf\"\n",
        "\n",
        "# Embedding Model für das Chunking festlegen.\n",
        "embedding_model=\"text-embedding-3-small\"\n",
        "\n",
        "# Eingabe der Chunk Größe (Token/Chunk)\n",
        "input_chunk_size = 300\n",
        "\n",
        "# Eingabe des Overlaps (Festlegen der Informationslücke)\n",
        "input_overlap = 50\n",
        "\n",
        "# Large Language Model für die Generierung der Antwort festlegen.\n",
        "large_language_model = \"gpt-4o-mini\"\n",
        "\n",
        "# Eingabe der Temperatur (Kreativität) der Antwort\n",
        "input_temperature = 0.3\n",
        "\n",
        "# Eingabe der maximalen Output-Token des LLM\n",
        "max_output_tokens = 300"
      ],
      "metadata": {
        "id": "GY6edzJ-EvlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets für den Zugriff auf den Google-Colab-KeyStore.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Auslesen des OpenAI-Key aus dem Google-Colab-KeyStore.\n",
        "OPENAI_API_KEY = userdata.get(colab_secret)\n",
        "\n",
        "# Auslesen des OpenWeather-Key aus dem Google-Colab-Keystore\n",
        "WEATHER_API_KEY= userdata.get(ow_colab_secret)"
      ],
      "metadata": {
        "id": "kiuODs-fEr6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereiten des PDF-Dokument für die Nutzung durch das RAG-Tool"
      ],
      "metadata": {
        "id": "pQx0lFHwFAlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der notwendigen Pakete für den Abruf des PDF-Dokument.\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Herunterladen der pdf Datei als Kontext für das RAG.\n",
        "response = requests.get(url)\n",
        "\n",
        "# Speichern des PDF-Byte-Stream.\n",
        "pdf_file = io.BytesIO(response.content)\n",
        "\n",
        "print(f\"Status Code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "evda2la9EysK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren von PyMuPDF um das PDF in Bilder zu verwandeln.\n",
        "import fitz\n",
        "\n",
        "# Array für die PDF-Bilder initialisieren.\n",
        "pdf_images = []\n",
        "\n",
        "# PDF-Datei öffnen.\n",
        "doc = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "\n",
        "# Über die Seiten des PDF-Dokument iterieren.\n",
        "for page_num in range(len(doc)):\n",
        "\n",
        "    # Seite des PDF-Dokument laden.\n",
        "    page = doc.load_page(page_num)\n",
        "\n",
        "    # PDF-Dokument als pixmap laden.\n",
        "    pix = page.get_pixmap(dpi=300)\n",
        "\n",
        "    # Pixmap in PNG konvertieren.\n",
        "    img_bytes = pix.tobytes(output=\"png\")\n",
        "\n",
        "    # Seite des PDF-Dokument zum Array hinzufügen.\n",
        "    pdf_images.append(img_bytes)\n",
        "\n",
        "# PDF-Dokument schließen.\n",
        "doc.close()\n",
        "\n",
        "print(f\"Seiten des PDF-Dokumentes: {len(pdf_images)}\")"
      ],
      "metadata": {
        "id": "878uBSXlFLLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um Text aus den PDF-Bildern auszulesen.\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Array für Textinhalte des PDF-Dokument initialisieren.\n",
        "extracted_text = []\n",
        "\n",
        "# Durch die Bilder des PDF-Dokument iterrieren.\n",
        "for image_bytes in pdf_images:\n",
        "\n",
        "    # Bild des PDF-Dokument öffnen.\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "    # Text aus dem Bild extrahieren.\n",
        "    text = pytesseract.image_to_string(image)\n",
        "\n",
        "    # Text aus dem Bild zum Array hinzufügen.\n",
        "    extracted_text.append(text)\n",
        "\n",
        "print(f\"Seiten mit erkanntem Text: {len(extracted_text)}\")"
      ],
      "metadata": {
        "id": "b5L80-LqFOaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable für den Text definieren.\n",
        "pdf_document = \"\"\n",
        "\n",
        "# Durch den erkannten Text iterrieren.\n",
        "for text in extracted_text:\n",
        "\n",
        "    # Text zusammenfügen.\n",
        "    pdf_document += text\n",
        "\n",
        "print(f\"Anzahl der erkannten Zeichen: {len(pdf_document)}\")"
      ],
      "metadata": {
        "id": "wRRFZ4MEFQ5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets um den Text aus dem PDF-Dokument in Chunks zu zerlegen.\n",
        "import tiktoken\n",
        "\n",
        "# Encoding für das genutzte Embedding-Modell abrufen,\n",
        "# dies hilft die Tokenanzahl zu kontrollieren und Kosten zu optimieren.\n",
        "encoding = tiktoken.encoding_for_model(embedding_model)\n",
        "\n",
        "# Das Dokument wird in Tokens zerlegt, welche die kleinste Einheit sind, die das LLM verarbeitet.\n",
        "# Das zerlegen in Token folgt dem speziellen Encoding für das jeweilige Embedding-Modell.\n",
        "# Die Token werden in diesem Fall benötigt, um das PDF-Dokument in Chunks zu zerlegen\n",
        "tokens = encoding.encode(pdf_document)\n",
        "\n",
        "# Diese Variable gibt an, wie viele Token maximal in jedem Chunk des PDF-Dokument sein dürfen.\n",
        "chunk_size = input_chunk_size\n",
        "\n",
        "# Diese Variable gibt an, wie viele Token in einem Chunk überlappen, um den Kontext besser zu erhalten.\n",
        "# Dadurch werden Informationslücken am Chunkrand vermieden.\n",
        "overlap = input_overlap\n",
        "\n",
        "# Array für die Chunks initialisieren.\n",
        "chunks = []\n",
        "\n",
        "# Kontrollvariable für die while-Schleife initialisieren.\n",
        "start = 0\n",
        "\n",
        "# Alle erstellten Tokens durchlaufen, bis alle Token einem Chunk zugeordnet wurden.\n",
        "while start < len(tokens):\n",
        "\n",
        "    # Speichern der für die chunk_size festgelegten Anzahl an Token in einen Chunk.\n",
        "    chunk = tokens[start:start + chunk_size]\n",
        "\n",
        "    # Den neuen Chunk zum Array aller Chunks hinzufügen.\n",
        "    chunks.append(encoding.decode(chunk))\n",
        "\n",
        "    # Bei den Token um die festgelegte overlap Anzahl zurückgehen, um den overlap zu berücksichtigen\n",
        "    # und Informationsverlust zu vermeiden.\n",
        "    start += chunk_size - overlap\n",
        "\n",
        "print(f\"Anzahl der Token: {len(tokens)}\")\n",
        "print(f\"Anzahl der Chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "n_ORr1kOFapv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um das Embedding-Modell aufzurufen, die Chunks-Liste in eine Documents-Liste zu verwandeln\n",
        "# und die Embeddings in einer Vektordatenbank zu speichern.\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Festlegen des Embedding-Modells, für die spätere Nutzung bei dem Embedding der Chunks.\n",
        "embeddings = OpenAIEmbeddings(\n",
        "\n",
        "    # Festlegen des Embedding-Modells.\n",
        "    model=embedding_model,\n",
        "\n",
        "    # Festlegen des API_KEY für OpenAI.\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# Verwandeln der Chunks-Liste in eine Documents-Liste, um diese als einen FAISS-Vektor speichern zu können.\n",
        "chunks_document = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# Embedding der Chunks und Speicherung in einem FAISS-Index für die Nutzung in der Chain.\n",
        "# In diesem Schritt werden die Chunks in das Embedding-Modell gegeben, welches die Chunks bzw. die Token\n",
        "# in Word-embeddings verwandelt, um diese für das LLM abrufbar zu machen.\n",
        "# FAISS steht für Facebook AI Similarity Search und ist eine Open-Source Bibliothek für die Vektorsuche,\n",
        "# es berechnet später in der Chain die Ähnlichkeit der Frage zu den gespeicherten Embeddings der Chunks\n",
        "# und gibt anschließend die relevantesten Chunks an das LLM weiter, welches basierend darauf die Antwort generiert.\n",
        "vectorstore = FAISS.from_documents(chunks_document, embeddings)"
      ],
      "metadata": {
        "id": "_Ty82PNsFdlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementierung der Tools die der Agent nutzen kann"
      ],
      "metadata": {
        "id": "tBd8fuvfT8Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des LangChain Pakets um die Tools für den Agenten zu definieren.\n",
        "from langchain.tools import tool\n",
        "\n",
        "# Definieren des ersten Tools: RAG-System.\n",
        "@tool\n",
        "def rag_tool(query: str) -> str:\n",
        "\n",
        "    # Beschreibung auf dessen Basis der Agent selbstständig entscheidet, bei welcher Art von Benutzereingabe das Tool verwendet werden soll.\n",
        "    \"\"\"Searches for relevant information in the loaded document based on the query.\"\"\"\n",
        "\n",
        "    # Auslesen der Top-3 zur Benutzereingabe passenden Chunks aus dem PDF-Dokument.\n",
        "    chunks = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    # Zusammenfügen der Chunks für die mitgabe im Kontext.\n",
        "    context = \"\\n\".join([c.page_content for c in chunks])\n",
        "\n",
        "    # Zusammenbauen des Kontext für die Antwort des LLM bei der Ausgabe.\n",
        "    context = f\"Beantworte die Frage ausschließlich basierend auf folgendem Kontext:\\n{context}\\nFrage: {query}. Wenn Du die Frage nicht beantworten kannst, antworte: Ich weiß es nicht\"\n",
        "\n",
        "    # Zurückgeben des Kontext, damit der Agent diesen an das LLM weitergeben kann.\n",
        "    return context\n",
        "\n",
        "# Definieren des zweiten Tools: Abfrage eine Wetter-API\n",
        "@tool\n",
        "def get_current_weather(location: str) -> str:\n",
        "\n",
        "    # Beschreibung auf dessen Basis der Agent selbstständig entscheidet, bei welcher Art von Benutzereingabe das Tool verwendet werden soll.\n",
        "    \"\"\"Fetches the current weather information for a given location.\"\"\"\n",
        "\n",
        "    # Zusammenbauen der URL für den Abruf der OpenWeather-API.\n",
        "    complete_url = WEATHER_BASE_URL + \"appid=\" + WEATHER_API_KEY + \"&q=\" + location + \"&units=metric\"\n",
        "\n",
        "    # Abfrage der OpenWeather-API.\n",
        "    response = requests.get(complete_url)\n",
        "\n",
        "    # Überprüfung ob die Abfrage der API erfolgreich war.\n",
        "    if response.status_code != 200:\n",
        "\n",
        "        # Rückgabe eines Fehler an das LLM, damit dieses eine passende Ausgabe generieren kann.\n",
        "        return f\"Error fetching weather data for {location}: {weather_data['message']}\"\n",
        "\n",
        "    # Speichern der API-Antwort im JSON-Format als Python-Objekt.\n",
        "    weather_data = response.json()\n",
        "\n",
        "    # Auslesen der Wetterdaten aus der API-Antwort.\n",
        "    main_data = weather_data[\"main\"]\n",
        "    temperature = main_data[\"temp\"]\n",
        "    humidity = main_data[\"humidity\"]\n",
        "    description = weather_data[\"weather\"][0][\"description\"]\n",
        "\n",
        "    # Zusammenbauen des Kontext für die Antwort des LLM bei der Ausgabe.\n",
        "    context = f\"Das Wetter in {location}: Temperatur - {temperature}°C, Luftfeuchtigkeit - {humidity}%, Beschreibung - {description}.\"\n",
        "\n",
        "    # Zurückgeben des Kontext, damit der Agent diesen an das LLM weitergeben kann.\n",
        "    return context"
      ],
      "metadata": {
        "id": "WncyhCQ7F8tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementierung einer unstrukturierten Antwort des Agenten"
      ],
      "metadata": {
        "id": "B1zaKzTUUBNQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdd330e0"
      },
      "source": [
        "# Importieren der LangChain Pakete, um den Agenten zu bauen und die Eingabe zu formatieren.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
        "\n",
        "# Festlegen des LLM für den Agenten, welcher selbstständig das passende Tool auswählt\n",
        "# und eine passende Antwort, basierend auf den durch das jeweilige Tool bereitgestellte Kontext, generiert.\n",
        "model = ChatOpenAI(\n",
        "\n",
        "    # Festlegen des Large-Language-Modells.\n",
        "    model=large_language_model,\n",
        "\n",
        "    # Festlegen des API_KEY für OpenAI.\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "\n",
        "    # Festlegen der Kreativität der Antworten.\n",
        "    # 0.3 liefert nicht immer die gleiche Antwort, aber ist faktenbasiert.\n",
        "    temperature=input_temperature,\n",
        "\n",
        "    # Maximale Anzahl der Ausgabetoken des Large Language Model, um die Kosten unter Kontrolle zu halten.\n",
        "    max_tokens=max_output_tokens\n",
        ")\n",
        "\n",
        "# Festlegen des Prompt, welcher an den Agenten übergeben wird, und eine Anweisung erhält, was der Agent tun soll.\n",
        "# Zudem wird die Eingabe des Benutzers entsprechend eingefügt.\n",
        "# Wichtig für Agentensysteme ist zudem den Gedankenprozess für eine Entscheidungsfindung in den Prompt einzubeten,\n",
        "# was im Falle der LangChain Prompt über {agent_scratchpad} passiert.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Du bist ein hilfreicher Assitent, der das aktuelle Wetter für einen Standort abruft, oder Informationen aus einem PDF-Dokument beantwortet.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "# Speichern der verfügbaren Tools in einer Liste, für die Übergabe an den Agenten.\n",
        "tools = [rag_tool,get_current_weather]\n",
        "\n",
        "# Initialisieren des Agent, der das model 'model' nutzt und auf die Tools 'tools' zugreifen kann und den Prompt 'prompt'\n",
        "# als Eingabe entgegennimmt um eine Tool-Entscheidung zu treffen und eine Antwort zu generieren.\n",
        "agent = create_tool_calling_agent(model, tools, prompt)\n",
        "\n",
        "# Erstellen eines AgentExecuter, der die Schritte des Agenten sowie die Eingabe, Tool-Nutzung und Ausgabe koordiniert.\n",
        "agent_unstructured = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Definieren einer LangChain lambda, um diese in der Chain verwenden zu können.\n",
        "format_input = RunnableLambda(\n",
        "\n",
        "    # Formatieren der Eingabe durch eine lambda für das Einfügen in den Prompt.\n",
        "    lambda user_input: {\"input\": user_input}\n",
        ")\n",
        "\n",
        "# Definieren der LangChain Chain, welche zuerst die Eingabe formatiert, diese in die Prompt einfügt\n",
        "# und anschließend den Agenten ausführt, welcher anhand der Prompt die Fragen beantwortet.\n",
        "chain = format_input | agent_unstructured\n",
        "\n",
        "# Anfrage an den Agent mit einer Frage, die er durch das PDF-Dokument aus dem RAG-System beantworten kann.\n",
        "chain.invoke(\"Was ist wichtig um effektiv mit MCP arbeiten zu können?\")\n",
        "\n",
        "# Anfrage an den Agenten, wie das Wetter in Stuttgart ist, was er durch die Abfrage der OpenWaether-API beantworten kann.\n",
        "chain.invoke(\"Wie ist das Wetter in Stuttgart?\")\n",
        "\n",
        "# Anfrage an den Agent, die er durch keins der Tools beantworten kann, und daher auf sein atrainiertes Wissen zurückgreifen muss.\n",
        "chain.invoke(\"Wer ist der aktuelle US-Präsident?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementierung einer strukturierten Antwort des Agenten"
      ],
      "metadata": {
        "id": "PpD4bQjuUFJq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad83b90a"
      },
      "source": [
        "# Importieren der Pakete um die Formatierungsanweisungen für die Ausgabe an den Agenten zu übergeben.\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Definieren eines Datenmodells das angibt, welcher Felder das JSON-Objekt haben soll und welche Antworten jeweils zugeordnet werden.\n",
        "class WeatherInfo(BaseModel):\n",
        "    location: str = Field(description=\"Der Name der Stadt\")\n",
        "    temperature: float = Field(description=\"Die aktuelle Temperatur in Grad Celsius\")\n",
        "    humidity: float = Field(description=\"Die aktuelle Luftfeuchtigkeit in Prozent\")\n",
        "    description: str = Field(description=\"Eine kurze Beschreibung der Wetterlage\")\n",
        "\n",
        "# Definieren des Ausgabe-Parser, um dem LLM mitzuteilen, wie es die Daten in der Antwort ausgeben soll.\n",
        "output_parser = PydanticOutputParser(pydantic_object=WeatherInfo)\n",
        "\n",
        "# Definieren der neuen Prompt, mit dem Platzhalter für die Formatierungsanweisungen.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Du bist ein hilfreicher Assitent, der das aktuelle Wetter für einen Standort abruft, oder Informationen aus einem PDF-Dokument beantwortet. Antworte nur mit den benötigten Informationen und formatieren die Ausgabe als JSON basierend auf der folgenden Anweisung:\\n{format_instructions}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "# Einfügen der Formatierungsanweisungen in den Prompt, um diese nicht bei jedem Aufruf mitgeben zu müssen.\n",
        "prompt = prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
        "\n",
        "# Initialisieren des Agent, der das model 'model' nutzt und auf die Tools 'tools' zugreifen kann und den Prompt 'prompt'\n",
        "# als Eingabe entgegennimmt um eine Tool-Entscheidung zu treffen und eine Antwort zu generieren.\n",
        "agent = create_tool_calling_agent(model, tools, prompt)\n",
        "\n",
        "# Erstellen eines AgentExecuter, der die Schritte des Agenten sowie die Eingabe, Tool-Nutzung und Ausgabe koordiniert.\n",
        "agent_structured = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Definieren einer LangChain lambda, um diese in der Chain verwenden zu können.\n",
        "format_input = RunnableLambda(\n",
        "\n",
        "    # Formatieren der Eingabe durch eine lambda für das Einfügen in den Prompt.\n",
        "    lambda user_input: {\"input\": user_input}\n",
        ")\n",
        "\n",
        "# Definieren einer LangChain lambda, um diese für das filtern des JSON-Objekt aus der Antwort des LLM verwenden zu können.\n",
        "extract_output = RunnableLambda(\n",
        "\n",
        "    # Extrahieren des JSON-Objekt aus der Antwort des LLM durch eine lambda.\n",
        "    lambda x: x.get('output', x)\n",
        ")\n",
        "\n",
        "# Definieren der LangChain Chain, welche zuerst die Eingabe formatiert, diese in die Prompt einfügt, den Agenten ausführt,\n",
        "# welcher anhand der Prompt das Wetter abruft und als strukturiertes JSON-Objekt zurückgibt, welches im letzten Schritt\n",
        "# aus der Antwort des LLM extrahiert wird.\n",
        "chain = format_input | agent_structured | extract_output\n",
        "\n",
        "# Anfrage an den Agent, wie das Wetter in Stuttgart ist, was er durch die Abfrage der OpenWaether-API beantworten kann.\n",
        "# Durch den Kontext aus der Prompt weiß er zudem, dass beim nennen einer Stadt, die Wetterinformationen abgerufen werden müssen.\n",
        "output = chain.invoke(\"Stuttgart\")\n",
        "\n",
        "# Ausgabe des formatierten JSON-Objekts mit den entsprechenden Werten.\n",
        "print(f\"Antwort des LLM im JSON-Format:\\n{output}\")\n",
        "\n",
        "# Anfrage an den Agent mit einer Frage, die er durch das PDF-Dokument aus dem RAG-System beantworten kann.\n",
        "# Hier liefert der Agent ebenfalls eine Antwort in Form eines JSON-Objekts zurück, jedoch nicht im beschriebenen Format,\n",
        "# da er selbsständig festgestellt hat, dass dieses Format nicht auf die Daten passt.\n",
        "# Stattdessen hat sich das LLM auf Basis seiner antrainierten Daten eine eigene Formatierung überlegt und die Ausgabe entsprechend formatiert.\n",
        "output = chain.invoke(\"Was ist wichtig um effektiv mit MCP arbeiten zu können?\")\n",
        "\n",
        "# Ausgabe des formatierten JSON-Objekts mit den entsprechenden Werten.\n",
        "print(f\"Antwort des LLM im JSON-Format:\\n{output}\")\n",
        "\n",
        "# Anfrage an den Agent, die er durch keins der Tools beantworten kann, und daher auf sein atrainiertes Wissen zurückgreifen muss.\n",
        "chain.invoke(\"Wie geht es dir?\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}