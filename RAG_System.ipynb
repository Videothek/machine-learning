{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJKZ5YFByFRv5s7DgVd+zF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Videothek/machine-learning/blob/main/RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation benötigter Pakete"
      ],
      "metadata": {
        "id": "_0bIKQFFgzN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTFtsIE2OaUX",
        "outputId": "a7304639-5a9d-4fd3-b2b7-a46178dfe421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.28)\n",
            "Requirement already satisfied: docarray in /usr/local/lib/python3.11/dist-packages (0.41.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.11/dist-packages (from docarray) (3.11.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from docarray) (2.11.7)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from docarray) (13.9.4)\n",
            "Requirement already satisfied: types-requests>=2.28.11.6 in /usr/local/lib/python3.11/dist-packages (from docarray) (2.32.4.20250611)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from docarray) (0.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->docarray) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->docarray) (1.1.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Installieren der notwendigen Pakete.\n",
        "!pip install PyMuPDF pytesseract Pillow langchain-community langchain_openai docarray\n",
        "!apt-get update\n",
        "!apt-get install tesseract-ocr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definieren relevanter Variablen"
      ],
      "metadata": {
        "id": "uCN-KvSxgYvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name des Google-Colab-Secret für den OpenAI-Key\n",
        "colab_secret = \"oai_apikey\"\n",
        "\n",
        "# Festlegen der URL von der das PDF-Dokument geladen werden kann.\n",
        "url = \"https://raw.githubusercontent.com/Videothek/machine-learning/main/MCP.pdf\"\n",
        "\n",
        "# Embedding Model für das Chunking festlegen.\n",
        "embedding_model=\"text-embedding-3-small\"\n",
        "\n",
        "# Eingabe der Chunk Größe (Token/Chunk)\n",
        "input_chunk_size = 300\n",
        "\n",
        "# Eingabe des Overlaps (Festlegen der Informationslücke)\n",
        "input_overlap = 50\n",
        "\n",
        "# Large Language Model für die Generierung der Antwort festlegen.\n",
        "large_language_model = \"gpt-4o-mini\"\n",
        "\n",
        "# Eingabe der maximalen Output-Token des LLM\n",
        "max_output_tokens = 300"
      ],
      "metadata": {
        "id": "MDiXVGd4fBUo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets für den Zugriff auf den Google-Colab-KeyStore.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Auslesen des OpenAI-Key aus dem Google-Colab-KeyStore.\n",
        "OPENAI_API_KEY = userdata.get(colab_secret)"
      ],
      "metadata": {
        "id": "TgUWRegbPFJT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereiten des PDF-Dokument"
      ],
      "metadata": {
        "id": "wiskq2pvglYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der notwendigen Pakete für den Abruf des PDF-Dokument.\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Herunterladen der pdf Datei als Kontext für das RAG.\n",
        "response = requests.get(url)\n",
        "\n",
        "# Speichern des PDF-Byte-Stream.\n",
        "pdf_file = io.BytesIO(response.content)\n",
        "\n",
        "print(f\"Status Code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyKt5v_9plZE",
        "outputId": "31c3356e-409e-4f47-b2b0-30895e460857"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status Code: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren von PyMuPDF um das PDF in Bilder zu verwandeln.\n",
        "import fitz\n",
        "\n",
        "# Array für die PDF-Bilder initialisieren.\n",
        "pdf_images = []\n",
        "\n",
        "# PDF-Datei öffnen.\n",
        "doc = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "\n",
        "# Über die Seiten des PDF-Dokument iterieren.\n",
        "for page_num in range(len(doc)):\n",
        "\n",
        "    # Seite des PDF-Dokument laden.\n",
        "    page = doc.load_page(page_num)\n",
        "\n",
        "    # PDF-Dokument als pixmap laden.\n",
        "    pix = page.get_pixmap(dpi=300)\n",
        "\n",
        "    # Pixmap in PNG konvertieren.\n",
        "    img_bytes = pix.tobytes(output=\"png\")\n",
        "\n",
        "    # Seite des PDF-Dokument zum Array hinzufügen.\n",
        "    pdf_images.append(img_bytes)\n",
        "\n",
        "# PDF-Dokument schließen.\n",
        "doc.close()\n",
        "\n",
        "print(f\"Seiten des PDF-Dokumentes: {len(pdf_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaQJprgLwUS1",
        "outputId": "acbdca3f-4aed-43dd-e5b4-ca0b5939054c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seiten des PDF-Dokumentes: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um Text aus den PDF-Bildern auszulesen.\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Array für Textinhalte des PDF-Dokument initialisieren.\n",
        "extracted_text = []\n",
        "\n",
        "# Durch die Bilder des PDF-Dokument iterrieren.\n",
        "for image_bytes in pdf_images:\n",
        "\n",
        "    # Bild des PDF-Dokument öffnen.\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "    # Text aus dem Bild extrahieren.\n",
        "    text = pytesseract.image_to_string(image)\n",
        "\n",
        "    # Text aus dem Bild zum Array hinzufügen.\n",
        "    extracted_text.append(text)\n",
        "\n",
        "print(f\"Seiten mit erkanntem Text: {len(extracted_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INOB_C9E0Ldl",
        "outputId": "6489ad15-722d-4f0d-bc8d-3b77956d7541"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seiten mit erkanntem Text: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable für den Text definieren.\n",
        "pdf_document = \"\"\n",
        "\n",
        "# Durch den erkannten Text iterrieren.\n",
        "for text in extracted_text:\n",
        "\n",
        "    # Text zusammenfügen.\n",
        "    pdf_document += text\n",
        "\n",
        "print(f\"Anzahl der erkannten Zeichen: {len(pdf_document)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m6P21ti23D0",
        "outputId": "5dbfaa8e-dc46-4a79-eda2-8193c71755e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl der erkannten Zeichen: 8178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementierung des RAG-Systems"
      ],
      "metadata": {
        "id": "GB9ABsvtgsAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets um den Text aus dem PDF-Dokument in Chunks zu zerlegen.\n",
        "import tiktoken\n",
        "\n",
        "# Encoding für das genutzte Embedding-Modell abrufen,\n",
        "# dies hilft die Tokenanzahl zu kontrollieren und Kosten zu optimieren.\n",
        "encoding = tiktoken.encoding_for_model(embedding_model)\n",
        "\n",
        "# Das Dokument wird in Tokens zerlegt, welche die kleinste Einheit sind, die das LLM verarbeitet.\n",
        "# Das zerlegen in Token folgt dem speziellen Encoding für das jeweilige Embedding-Modell.\n",
        "# Die Token werden in diesem Fall benötigt, um das PDF-Dokument in Chunks zu zerlegen\n",
        "tokens = encoding.encode(pdf_document)\n",
        "\n",
        "# Diese Variable gibt an, wie viele Token maximal in jedem Chunk des PDF-Dokument sein dürfen.\n",
        "chunk_size = input_chunk_size\n",
        "\n",
        "# Diese Variable gibt an, wie viele Token in einem Chunk überlappen, um den Kontext besser zu erhalten.\n",
        "# Dadurch werden Informationslücken am Chunkrand vermieden.\n",
        "overlap = input_overlap\n",
        "\n",
        "# Array für die Chunks initialisieren.\n",
        "chunks = []\n",
        "\n",
        "# Kontrollvariable für die while-Schleife initialisieren.\n",
        "start = 0\n",
        "\n",
        "# Alle erstellten Tokens durchlaufen, bis alle Token einem Chunk zugeordnet wurden.\n",
        "while start < len(tokens):\n",
        "\n",
        "    # Speichern der für die chunk_size festgelegten Anzahl an Token in einen Chunk.\n",
        "    chunk = tokens[start:start + chunk_size]\n",
        "\n",
        "    # Den neuen Chunk zum Array aller Chunks hinzufügen.\n",
        "    chunks.append(encoding.decode(chunk))\n",
        "\n",
        "    # Bei den Token um die festgelegte overlap Anzahl zurückgehen, um den overlap zu berücksichtigen\n",
        "    # und Informationsverlust zu vermeiden.\n",
        "    start += chunk_size - overlap\n",
        "\n",
        "print(f\"Anzahl der Token: {len(tokens)}\")\n",
        "print(f\"Anzahl der Chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLOL9IzxCLHB",
        "outputId": "ed585c5d-433f-46c7-a870-57a8accca4be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl der Token: 2356\n",
            "Anzahl der Chunks: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um das Embedding-Modell aufzurufen, die Chunks-Liste in eine Documents-Liste zu verwandeln\n",
        "# und die Embeddings in einer Vektordatenbank zu speichern.\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Festlegen des Embedding-Modells, für die spätere Nutzung bei dem Embedding der Chunks.\n",
        "embeddings = OpenAIEmbeddings(\n",
        "\n",
        "    # Festlegen des Embedding-Modells.\n",
        "    model=embedding_model,\n",
        "\n",
        "    # Festlegen des API_KEY für OpenAI.\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# Verwandeln der Chunks-Liste in eine Documents-Liste, um diese als einen FAISS-Vektor speichern zu können.\n",
        "chunks_document = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# Embedding der Chunks und Speicherung in einem FAISS-Index für die Nutzung in der Chain.\n",
        "# In diesem Schritt werden die Chunks in das Embedding-Modell gegeben, welches die Chunks bzw. die Token\n",
        "# in Word-embeddings verwandelt, um diese für das LLM abrufbar zu machen.\n",
        "# FAISS steht für Facebook AI Similarity Search und ist eine Open-Source Bibliothek für die Vektorsuche,\n",
        "# es berechnet später in der Chain die Ähnlichkeit der Frage zu den gespeicherten Word-Embeddings der Chunks\n",
        "# und gibt die relevantesten Text-Passagen an das LLM weiter, welches basierend darauf die Antwort generiert.\n",
        "vectorstore = FAISS.from_documents(chunks_document, embeddings)"
      ],
      "metadata": {
        "id": "OKsM9YN15Uyl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um das LLM abzurufen, den Antwort-String aus der OpenAI-Antwort zu extrahieren\n",
        "# und das Chat-Template zu bauen.\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Festlegen des LLM für die Generierung der Antwort in der Chain basierend auf dem Kontext.\n",
        "model = ChatOpenAI(\n",
        "\n",
        "    # Festlegen des Large-Language-Modells.\n",
        "    model=large_language_model,\n",
        "\n",
        "    # Festlegen des API_KEY für OpenAI.\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "\n",
        "    # Maximale Anzahl der Ausgabetoken des Large Language Model, um die Kosten unter Kontrolle zu behalten.\n",
        "    max_tokens=max_output_tokens\n",
        ")\n",
        "\n",
        "# Festlegen des Parsers, um die Antwort als String aus dem komplexen Objekt auszulesen,\n",
        "# welches durch das LLM bzw. OpenAI ausgegeben wird.\n",
        "# Hierbei handelt es sich um den Standardparser für Langchain chains und kann beispielsweise\n",
        "# durch einen JsonParser ausgetauscht werden, um JSON-Strukturierte Ausgaben zu erhalten.\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Festlegen des Template, welches genutzt wird, um den Kontext und die Frage an das LLM zu geben.\n",
        "# Es vereinfacht die Mitgabe von Anweisungen an das LLM, wie es zum Beispiel zu antworten hat,\n",
        "# wenn die Informationen nicht durch das Dokument beantwortet werden können.\n",
        "# {context} und {question} sind Platzhalter für die in der chain definierte Paramter.\n",
        "template = \"\"\"\n",
        "Beantworte die Frage basierend auf dem Kontext.\n",
        "Wenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Vorbereitung eines LangChain Prompt-Objekts um dieses später mit dem Kontext und der Frage\n",
        "# füllen zu können.\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "38Vj4vj87V4Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets um die Eingabe der Frage direkt an die Chain, und damit auch den Prompt, weiterzugeben.\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Definieren der Chain, welche dem Model den zur Frage passenden Kontext aus der FAISS Verktordatenbank,\n",
        "# sowie die Frage mitgibt, die als Eingabe direkt weitergelitet wird, mitgibt.\n",
        "# Zudem wird der finale prompt gebaut, indem der Kontext und die Frage in das zuvor gebaute Template eingefügt wird.\n",
        "# Anschließend wird das definierte LLM aufgerufen, welches die prompt erhält\n",
        "# und die Frage anhand des Kontext beantworten soll.\n",
        "# Anschließend wird die Ausgabe des LLM durch den Parser umgewandelt,\n",
        "# um den reinen Antwort-String zu erhalten.\n",
        "# Durch diese Formatierung und Vorgehen, wird die Komplexität einer solchen AI-Chain zusammengefasst und\n",
        "# übersichtlich dargestellt.\n",
        "chain = (\n",
        "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "\n",
        "# Letztendlich wird die zuvor definierte Chain ausgeführt. Die Frage wird dabei, wie zuvor beschrieben,\n",
        "# direkt in das Template eingebaut.\n",
        "# Der Retriever durchsucht anschließend die Vektordatenbank nach passenden Textpassagen und gibt\n",
        "# diese an das LLM weiter, welche basierend auf der Frage und den Textpassagen eine Antwort generiert.\n",
        "# Anschließend wird die Antwort durch den Parser aus dem zurückgelieferten Objekt gefiltert und ausgegeben.\n",
        "chain.invoke(\"Was ist wichtig um effektiv mit MCP arbeiten zu können?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tsdmo2QX644o",
        "outputId": "6f1d9d26-c333-4e19-b65a-1bb667eb4426"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Um effektiv mit MCP arbeiten zu können, ist es wichtig, die grundlegenden Konzepte und die damit verbundene Terminologie zu verstehen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}