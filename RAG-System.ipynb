{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJKZ5YFByFRv5s7DgVd+zF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Videothek/machine-learning/blob/main/RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation benötigter Pakete"
      ],
      "metadata": {
        "id": "_0bIKQFFgzN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTFtsIE2OaUX"
      },
      "outputs": [],
      "source": [
        "# Installieren der notwendigen Pakete.\n",
        "!pip install PyMuPDF pytesseract Pillow langchain-community langchain_openai docarray\n",
        "!apt-get update\n",
        "!apt-get install tesseract-ocr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definieren relevanter Variablen"
      ],
      "metadata": {
        "id": "uCN-KvSxgYvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name des Google-Colab-Secret für den OpenAI-Key\n",
        "colab_secret = \"oai_apikey\"\n",
        "\n",
        "# Festlegen der URL von der das PDF-Dokument geladen werden kann.\n",
        "url = \"https://raw.githubusercontent.com/Videothek/machine-learning/main/MCP.pdf\"\n",
        "\n",
        "# Embedding Model für das Chunking festlegen.\n",
        "embedding_model=\"text-embedding-3-small\"\n",
        "\n",
        "# Eingabe der Chunk Größe (Token/Chunk)\n",
        "input_chunk_size = 300\n",
        "\n",
        "# Eingabe des Overlaps (Festlegen der Informationslücke)\n",
        "input_overlap = 50\n",
        "\n",
        "# Large Language Model für die Generierung der Antwort festlegen.\n",
        "large_language_model = \"gpt-4o-mini\"\n",
        "\n",
        "# Eingabe der maximalen Output-Token des LLM\n",
        "max_output_tokens = 300"
      ],
      "metadata": {
        "id": "MDiXVGd4fBUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets für den Zugriff auf den Google-Colab-KeyStore.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Auslesen des OpenAI-Key aus dem Google-Colab-KeyStore.\n",
        "OPENAI_API_KEY = userdata.get(colab_secret)"
      ],
      "metadata": {
        "id": "TgUWRegbPFJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vorbereiten des PDF-Dokument"
      ],
      "metadata": {
        "id": "wiskq2pvglYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der notwendigen Pakete für den Abruf des PDF-Dokument.\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Herunterladen der pdf Datei als Kontext für das RAG.\n",
        "response = requests.get(url)\n",
        "\n",
        "# Speichern des PDF-Byte-Stream.\n",
        "pdf_file = io.BytesIO(response.content)\n",
        "\n",
        "print(f\"Status Code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "pyKt5v_9plZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren von PyMuPDF um das PDF in Bilder zu verwandeln.\n",
        "import fitz\n",
        "\n",
        "# Array für die PDF-Bilder initialisieren.\n",
        "pdf_images = []\n",
        "\n",
        "# PDF-Datei öffnen.\n",
        "doc = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "\n",
        "# Über die Seiten des PDF-Dokument iterieren.\n",
        "for page_num in range(len(doc)):\n",
        "\n",
        "    # Seite des PDF-Dokument laden.\n",
        "    page = doc.load_page(page_num)\n",
        "\n",
        "    # PDF-Dokument als pixmap laden.\n",
        "    pix = page.get_pixmap(dpi=300)\n",
        "\n",
        "    # Pixmap in PNG konvertieren.\n",
        "    img_bytes = pix.tobytes(output=\"png\")\n",
        "\n",
        "    # Seite des PDF-Dokument zum Array hinzufügen.\n",
        "    pdf_images.append(img_bytes)\n",
        "\n",
        "# PDF-Dokument schließen.\n",
        "doc.close()\n",
        "\n",
        "print(f\"Seiten des PDF-Dokumentes: {len(pdf_images)}\")"
      ],
      "metadata": {
        "id": "AaQJprgLwUS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um Text aus den PDF-Bildern auszulesen.\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Array für Textinhalte des PDF-Dokument initialisieren.\n",
        "extracted_text = []\n",
        "\n",
        "# Durch die Bilder des PDF-Dokument iterrieren.\n",
        "for image_bytes in pdf_images:\n",
        "\n",
        "    # Bild des PDF-Dokument öffnen.\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "    # Text aus dem Bild extrahieren.\n",
        "    text = pytesseract.image_to_string(image)\n",
        "\n",
        "    # Text aus dem Bild zum Array hinzufügen.\n",
        "    extracted_text.append(text)\n",
        "\n",
        "print(f\"Seiten mit erkanntem Text: {len(extracted_text)}\")"
      ],
      "metadata": {
        "id": "INOB_C9E0Ldl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable für den Text definieren.\n",
        "pdf_document = \"\"\n",
        "\n",
        "# Durch den erkannten Text iterrieren.\n",
        "for text in extracted_text:\n",
        "\n",
        "    # Text zusammenfügen.\n",
        "    pdf_document += text\n",
        "\n",
        "print(f\"Anzahl der erkannten Zeichen: {len(pdf_document)}\")"
      ],
      "metadata": {
        "id": "8m6P21ti23D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementierung des RAG-Systems"
      ],
      "metadata": {
        "id": "GB9ABsvtgsAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets um den Text aus dem PDF-Dokument in Chunks zu zerlegen.\n",
        "import tiktoken\n",
        "\n",
        "# Encoding für das genutzte Embedding-Modell abrufen,\n",
        "# dies hilft die Tokenanzahl zu kontrollieren und Kosten zu optimieren.\n",
        "encoding = tiktoken.encoding_for_model(embedding_model)\n",
        "\n",
        "# Das Dokument wird in Tokens zerlegt, welche die kleinste Einheit sind, die das LLM verarbeitet.\n",
        "# Das zerlegen in Token folgt dem speziellen Encoding für das jeweilige Embedding-Modell.\n",
        "# Die Token werden in diesem Fall benötigt, um das PDF-Dokument in Chunks zu zerlegen\n",
        "tokens = encoding.encode(pdf_document)\n",
        "\n",
        "# Diese Variable gibt an, wie viele Token maximal in jedem Chunk des PDF-Dokument sein dürfen.\n",
        "chunk_size = input_chunk_size\n",
        "\n",
        "# Diese Variable gibt an, wie viele Token in einem Chunk überlappen, um den Kontext besser zu erhalten.\n",
        "# Dadurch werden Informationslücken am Chunkrand vermieden.\n",
        "overlap = input_overlap\n",
        "\n",
        "# Array für die Chunks initialisieren.\n",
        "chunks = []\n",
        "\n",
        "# Kontrollvariable für die while-Schleife initialisieren.\n",
        "start = 0\n",
        "\n",
        "# Alle erstellten Tokens durchlaufen, bis alle Token einem Chunk zugeordnet wurden.\n",
        "while start < len(tokens):\n",
        "\n",
        "    # Speichern der für die chunk_size festgelegten Anzahl an Token in einen Chunk.\n",
        "    chunk = tokens[start:start + chunk_size]\n",
        "\n",
        "    # Den neuen Chunk zum Array aller Chunks hinzufügen.\n",
        "    chunks.append(encoding.decode(chunk))\n",
        "\n",
        "    # Bei den Token um die festgelegte overlap Anzahl zurückgehen, um den overlap zu berücksichtigen\n",
        "    # und Informationsverlust zu vermeiden.\n",
        "    start += chunk_size - overlap\n",
        "\n",
        "print(f\"Anzahl der Token: {len(tokens)}\")\n",
        "print(f\"Anzahl der Chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "oLOL9IzxCLHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um das Embedding-Modell aufzurufen, die Chunks-Liste in eine Documents-Liste zu verwandeln\n",
        "# und die Embeddings in einer Vektordatenbank zu speichern.\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Festlegen des Embedding-Modells, für die spätere Nutzung bei dem Embedding der Chunks.\n",
        "embeddings = OpenAIEmbeddings(\n",
        "\n",
        "    # Festlegen des Embedding-Modells.\n",
        "    model=embedding_model,\n",
        "\n",
        "    # Festlegen des API_KEY für OpenAI.\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# Verwandeln der Chunks-Liste in eine Documents-Liste, um diese als einen FAISS-Vektor speichern zu können.\n",
        "chunks_document = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# Embedding der Chunks und Speicherung in einem FAISS-Index für die Nutzung in der Chain.\n",
        "# In diesem Schritt werden die Chunks in das Embedding-Modell gegeben, welches die Chunks bzw. die Token\n",
        "# in Word-embeddings verwandelt, um diese für das LLM abrufbar zu machen.\n",
        "# FAISS steht für Facebook AI Similarity Search und ist eine Open-Source Bibliothek für die Vektorsuche,\n",
        "# es berechnet später in der Chain die Ähnlichkeit der Frage zu den gespeicherten Word-Embeddings der Chunks\n",
        "# und gibt die relevantesten Text-Passagen an das LLM weiter, welches basierend darauf die Antwort generiert.\n",
        "vectorstore = FAISS.from_documents(chunks_document, embeddings)"
      ],
      "metadata": {
        "id": "OKsM9YN15Uyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren der Pakete um das LLM abzurufen, den Antwort-String aus der OpenAI-Antwort zu extrahieren\n",
        "# und das Chat-Template zu bauen.\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Festlegen des LLM für die Generierung der Antwort in der Chain basierend auf dem Kontext.\n",
        "model = ChatOpenAI(\n",
        "\n",
        "    # Festlegen des Large-Language-Modells.\n",
        "    model=large_language_model,\n",
        "\n",
        "    # Festlegen des API_KEY für OpenAI.\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "\n",
        "    # Maximale Anzahl der Ausgabetoken des Large Language Model, um die Kosten unter Kontrolle zu behalten.\n",
        "    max_tokens=max_output_tokens\n",
        ")\n",
        "\n",
        "# Festlegen des Parsers, um die Antwort als String aus dem komplexen Objekt auszulesen,\n",
        "# welches durch das LLM bzw. OpenAI ausgegeben wird.\n",
        "# Hierbei handelt es sich um den Standardparser für Langchain chains und kann beispielsweise\n",
        "# durch einen JsonParser ausgetauscht werden, um JSON-Strukturierte Ausgaben zu erhalten.\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Festlegen des Template, welches genutzt wird, um den Kontext und die Frage an das LLM zu geben.\n",
        "# Es vereinfacht die Mitgabe von Anweisungen an das LLM, wie es zum Beispiel zu antworten hat,\n",
        "# wenn die Informationen nicht durch das Dokument beantwortet werden können.\n",
        "# {context} und {question} sind Platzhalter für die in der chain definierte Paramter.\n",
        "template = \"\"\"\n",
        "Beantworte die Frage basierend auf dem Kontext.\n",
        "Wenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Vorbereitung eines LangChain Prompt-Objekts um dieses später mit dem Kontext und der Frage\n",
        "# füllen zu können.\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "38Vj4vj87V4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importieren des Pakets um die Eingabe der Frage direkt an die Chain, und damit auch den Prompt, weiterzugeben.\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Definieren der Chain, welche dem Model den zur Frage passenden Kontext aus der FAISS Verktordatenbank,\n",
        "# sowie die Frage mitgibt, die als Eingabe direkt weitergelitet wird, mitgibt.\n",
        "# Zudem wird der finale prompt gebaut, indem der Kontext und die Frage in das zuvor gebaute Template eingefügt wird.\n",
        "# Anschließend wird das definierte LLM aufgerufen, welches die prompt erhält\n",
        "# und die Frage anhand des Kontext beantworten soll.\n",
        "# Anschließend wird die Ausgabe des LLM durch den Parser umgewandelt,\n",
        "# um den reinen Antwort-String zu erhalten.\n",
        "# Durch diese Formatierung und Vorgehen, wird die Komplexität einer solchen AI-Chain zusammengefasst und\n",
        "# übersichtlich dargestellt.\n",
        "chain = (\n",
        "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "\n",
        "# Letztendlich wird die zuvor definierte Chain ausgeführt. Die Frage wird dabei, wie zuvor beschrieben,\n",
        "# direkt in das Template eingebaut.\n",
        "# Der Retriever durchsucht anschließend die Vektordatenbank nach passenden Textpassagen und gibt\n",
        "# diese an das LLM weiter, welche basierend auf der Frage und den Textpassagen eine Antwort generiert.\n",
        "# Anschließend wird die Antwort durch den Parser aus dem zurückgelieferten Objekt gefiltert und ausgegeben.\n",
        "chain.invoke(\"Was ist wichtig um effektiv mit MCP arbeiten zu können?\")"
      ],
      "metadata": {
        "id": "Tsdmo2QX644o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
